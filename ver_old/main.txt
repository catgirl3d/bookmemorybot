import chromadb
from utils import split_text, embed_texts, ask_gpt
from pathlib import Path
import os
from tqdm import tqdm
import pickle
import sys
import hashlib

# Настройка клиента ChromaDB
client = chromadb.PersistentClient(path="db/")

def expand_by_indices(idxs: list[int], all_chunks: list[str], window: int = 1) -> list[str]:
    result_idxs = set()
    for idx in idxs:
        result_idxs.add(idx)
        for offset in (-window, window):
            ni = idx + offset
            if 0 <= ni < len(all_chunks):
                result_idxs.add(ni)
    return [all_chunks[i] for i in sorted(result_idxs)]

# Поиск всех текстовых файлов в папке data
data_path = Path("data/")
text_files = list(data_path.glob("*.txt"))
assert text_files, "Положи хотя бы один .txt-файл в папку data/"

# Проверка и обработка книг
def get_file_hash(file_path):
    with open(file_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

hash_path = Path("data/file_hashes.pkl")
if hash_path.exists():
    with open(hash_path, "rb") as f:
        file_hashes = pickle.load(f)
else:
    file_hashes = {}

all_chunks_map = {}

print("\nДоступные файлы:")
available_files = [f.name for f in text_files]
for name in available_files:
    print("-", name)

existing_sources = set(available_files)
target_file = input("\nВведите имя файла, по которому искать (или оставьте пустым для всех): ").strip()

files_to_process = []
if target_file:
    file = data_path / target_file
    if file.exists():
        files_to_process = [file]
    else:
        print("Файл не найден. Будет выполнен поиск по всем книгам.")
        target_file = ""
        files_to_process = text_files
else:
    files_to_process = text_files

for file_path in files_to_process:
    print(f"\nОбработка файла: {file_path.name}")
    print("Загрузка и анализ файла, пожалуйста подождите...")
    print("Шаг 1: Проверка хэша и состояния коллекции...")
    file_hash = get_file_hash(file_path)
    collection_name = file_path.stem.replace(" ", "_").lower()

    if file_path.name in file_hashes and file_hashes[file_path.name] != file_hash:
        print("Файл изменён — коллекция будет пересоздана.")
        client.delete_collection(collection_name)
    elif file_path.name in file_hashes:
        print("Файл не изменён. Пропуск загрузки.")
        print(f"Шаг 2: Чтение текста из файла {file_path.name}...")
        try:
            file_size = file_path.stat().st_size / (1024 * 1024)  # Размер в МБ
            print(f"Размер файла: {file_size:.2f} МБ")
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
                print(f"Прочитано {len(text)} символов")
        except UnicodeDecodeError:
            print(f"Ошибка: Файл {file_path.name} не в кодировке UTF-8.")
            sys.exit(1)
        except IOError as e:
            print(f"Ошибка при чтении файла {file_path.name}: {e}")
            sys.exit(1)
        print("✔ Шаг 2 завершён")
        all_chunks_map[file_path.name] = split_text(text)
        continue

    file_hashes[file_path.name] = file_hash

    print(f"Шаг 2: Чтение текста из файла {file_path.name}...")
    try:
        file_size = file_path.stat().st_size / (1024 * 1024)  # Размер в МБ
        print(f"Размер файла: {file_size:.2f} МБ")
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
            print(f"Прочитано {len(text)} символов")
    except UnicodeDecodeError:
        print(f"Ошибка: Файл {file_path.name} не в кодировке UTF-8.")
        sys.exit(1)
    except IOError as e:
        print(f"Ошибка при чтении файла {file_path.name}: {e}")
        sys.exit(1)
    print("✔ Шаг 2 завершён")

    print("Шаг 3: Разбиение текста на чанки...")
    chunks = split_text(text)
    print("✔ Шаг 3 завершён")
    all_chunks_map[file_path.name] = chunks
    cache_path = file_path.with_suffix(".pkl")
    embeddings = []

    print("Шаг 4: Загрузка или создание эмбеддингов...")
    if cache_path.exists():
        with open(cache_path, "rb") as f:
            embeddings = pickle.load(f)
        print("Загружены эмбеддинги из кэша.")
    else:
        retry_delay = 1
        for i in tqdm(range(0, len(chunks), 10), desc="Создание эмбеддингов"):
            batch = chunks[i:i+10]
            while True:
                try:
                    batch_embeddings = embed_texts(batch)
                    embeddings.extend(batch_embeddings)
                    with open(cache_path, "wb") as f:
                        pickle.dump(embeddings, f)
                    retry_delay = 1
                    break
                except Exception as e:
                    print(f"Ошибка при создании эмбеддингов: {e}. Повтор через {retry_delay} секунд...")
                    time.sleep(retry_delay)
                    retry_delay = min(retry_delay * 2, 60)

        print("Эмбеддинги сохранены в кэш.")
    print("✔ Шаг 4 завершён")

    print("Шаг 5: Сохранение чанков в базу данных...")
    collection = client.get_or_create_collection(name=collection_name)
    existing_ids = set(collection.get(ids=None)["ids"])

    for i, chunk in enumerate(tqdm(chunks, desc="Сохранение в базу")):
        doc_id = f"{file_path.stem}_chunk_{i}"
        if doc_id in existing_ids:
            continue
        collection.add(
            documents=[chunk],
            embeddings=[embeddings[i]],
            ids=[doc_id]
        )
    print("✔ Шаг 5 завершён")

with open("data/file_hashes.pkl", "wb") as f:
    pickle.dump(file_hashes, f)

while True:
    query = input("\nВведите запрос (или 'exit' для выхода): ")
    if query.strip().lower() == "exit":
        print("Выход из программы.")
        break

    query_embedding_result = embed_texts([query])
    if not query_embedding_result:
        print("Ошибка: не удалось получить эмбеддинг запроса. Проверь подключение к API.")
        continue

    query_embed = query_embedding_result[0]

    collections_to_query = []
    if target_file:
        collection_name = Path(target_file).stem.replace(" ", "_").lower()
        collections_to_query = [client.get_or_create_collection(name=collection_name)]
        files_for_query = [target_file]
    else:
        collections_to_query = [client.get_or_create_collection(name=Path(f.name).stem.replace(" ", "_").lower()) for f in text_files]
        files_for_query = [f.name for f in text_files]

    relevant_chunks = []
    for col, fname in zip(collections_to_query, files_for_query):
        results = col.query(
            query_embeddings=[query_embed],
            n_results=1,
            include=["documents"]
        )

        ids = results["ids"][0]
        idxs = [int(doc_id.rsplit("_", 1)[-1]) for doc_id in ids]

        chunks = all_chunks_map.get(fname, [])
        neighbor_docs = expand_by_indices(idxs, chunks, window=1)
        relevant_chunks.extend(neighbor_docs)

    print("\nРелевантные фрагменты:")
    if not relevant_chunks:
        print("Нет релевантных фрагментов.")
    else:
        for idx, doc in enumerate(relevant_chunks, 1):
            print(f"\n--- Фрагмент {idx} ---\n{doc}")

    # answer = ask_gpt(relevant_chunks, query)
    # print("\nОтвет GPT:")
    # print(answer)